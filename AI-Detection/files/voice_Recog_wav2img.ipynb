{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(r'D:\\ML\\minor project')\n",
        "print(\"Current working directory:\", os.getcwd())"
      ],
      "metadata": {
        "id": "_GcFG53J4uyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8a2c29-9063-4078-8d3f-52e710c8053a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: D:\\ML\\minor project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Audio Deepfake Detection System - Spectrogram Image Version\n",
        "\"\"\"\n",
        "\n",
        "# %% Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# %% Configuration\n",
        "AUDIO_PATH = r\"D:\\ML\\minor project\\archive\\KAGGLE\\AUDIO\"\n",
        "SPECTROGRAM_PATH = r\"D:\\ML\\minor project\\archive\\KAGGLE\\SPECTROGRAMS\"\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "CLASS_NAMES = ['FAKE', 'REAL']\n",
        "\n",
        "# Create new spectrogram directories\n",
        "for label in CLASS_NAMES:\n",
        "    os.makedirs(os.path.join(SPECTROGRAM_PATH, label), exist_ok=True)\n",
        "\n",
        "# %% Spectrogram Generation Function\n",
        "def create_spectrogram(audio_path, save_path):\n",
        "    \"\"\"Convert audio file to Mel-spectrogram and save as image\"\"\"\n",
        "    y, sr = librosa.load(audio_path, sr=22050)\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    plt.figure(figsize=(2.24, 2.24), frameon=False)\n",
        "    librosa.display.specshow(S_dB, sr=sr)\n",
        "    plt.axis('off')\n",
        "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "# %% Dataset Preparation (Run once)\n",
        "def prepare_dataset():\n",
        "    \"\"\"Convert all audio files to spectrogram images\"\"\"\n",
        "    for label in CLASS_NAMES:\n",
        "        audio_dir = os.path.join(AUDIO_PATH, label)\n",
        "        img_dir = os.path.join(SPECTROGRAM_PATH, label)  # Changed to new path\n",
        "\n",
        "        for file_name in os.listdir(audio_dir):\n",
        "            if file_name.endswith('.wav'):\n",
        "                audio_path = os.path.join(audio_dir, file_name)\n",
        "                img_name = f\"{os.path.splitext(file_name)[0]}.png\"\n",
        "                img_path = os.path.join(img_dir, img_name)  # Save to new location\n",
        "                create_spectrogram(audio_path, img_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prepare_dataset()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m59s6Zlj4Dq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39c8243e-dd82-4fd9-d369-be8be4bcf97f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
            "  \"cipher\": algorithms.TripleDES,\n",
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
            "  \"class\": algorithms.Blowfish,\n",
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
            "  \"class\": algorithms.TripleDES,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Enhanced Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten,\n",
        "                                   Dense, Dropout, BatchNormalization)\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.utils import class_weight\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "# %% Configuration\n",
        "AUDIO_PATH = r\"D:\\ML\\minor project\\archive\\KAGGLE\\AUDIO\"\n",
        "SPECTROGRAM_PATH = r\"D:\\ML\\minor project\\archive\\KAGGLE\\SPECTROGRAMS\"\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 16  # Reduced for smaller datasets\n",
        "EPOCHS = 100\n",
        "CLASS_NAMES = ['FAKE', 'REAL']\n",
        "\n",
        "# %% Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=45,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    brightness_range=[0.8,1.2],\n",
        "    fill_mode='constant',\n",
        "    cval=0,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Validation datagen (no augmentation)\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# %% Data Generators (Corrected)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    SPECTROGRAM_PATH,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    classes=CLASS_NAMES,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    SPECTROGRAM_PATH,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    classes=CLASS_NAMES,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# %% Enhanced Class Weight Calculation\n",
        "def calculate_class_weights():\n",
        "    class_counts = {}\n",
        "    for label in CLASS_NAMES:\n",
        "        class_dir = os.path.join(SPECTROGRAM_PATH, label)\n",
        "        class_counts[label] = len([f for f in os.listdir(class_dir) if f.endswith('.png')])\n",
        "\n",
        "    y_train = []\n",
        "    for label in CLASS_NAMES:\n",
        "        y_train += [CLASS_NAMES.index(label)] * class_counts[label]\n",
        "\n",
        "    weights = class_weight.compute_class_weight(\n",
        "        'balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "    return {i: w for i, w in enumerate(weights)}\n",
        "\n",
        "CLASS_WEIGHTS = calculate_class_weights()\n",
        "print(f\"Class weights: {CLASS_WEIGHTS}\")\n",
        "\n",
        "# %% Simplified CNN Model\n",
        "def create_cnn_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(8, (3,3), activation='relu', padding='same', input_shape=(*IMG_SIZE, 3)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2,2),\n",
        "\n",
        "        Conv2D(16, (3,3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2,2),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-4),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# %% Training Configuration\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_recall',  # Focus on minority class recall\n",
        "    patience=20,\n",
        "    min_delta=0.001,\n",
        "    mode='max',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max'\n",
        ")\n",
        "\n",
        "# %% Enhanced Evaluation Function\n",
        "def evaluate_model():\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        SPECTROGRAM_PATH,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        shuffle=False,\n",
        "        classes=CLASS_NAMES\n",
        "    )\n",
        "\n",
        "    y_true = test_generator.classes\n",
        "    y_pred_probs = model.predict(test_generator)\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=CLASS_NAMES, zero_division=0))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "    # ROC Curve\n",
        "    if len(np.unique(y_true)) > 1:  # Only plot if both classes present\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_pred_probs)\n",
        "        roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
        "        plt.figure()\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "                 label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.savefig('roc_curve.png')\n",
        "        plt.close()\n",
        "\n",
        "    return model.evaluate(test_generator)\n",
        "\n",
        "# %% Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Dataset verification\n",
        "    print(\"Class distribution:\")\n",
        "    class_counts = {}\n",
        "    for label in CLASS_NAMES:\n",
        "        path = os.path.join(SPECTROGRAM_PATH, label)\n",
        "        count = len([f for f in os.listdir(path) if f.endswith('.png')])\n",
        "        class_counts[label] = count\n",
        "        print(f\"{label}: {count} samples\")\n",
        "\n",
        "    # Handle extreme imbalance\n",
        "    if abs(class_counts['FAKE'] - class_counts['REAL']) > 10:\n",
        "        print(\"\\nWarning: Severe class imbalance detected!\")\n",
        "        print(\"Consider data augmentation for minority class or collecting more samples\")\n",
        "\n",
        "    # Model initialization\n",
        "    model = create_cnn_model()\n",
        "\n",
        "    # Calculate steps\n",
        "    steps_per_epoch = max(1, train_generator.samples // BATCH_SIZE)\n",
        "    val_steps = max(1, val_generator.samples // BATCH_SIZE)\n",
        "\n",
        "    # Training\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_generator,\n",
        "        validation_steps=val_steps,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[early_stop, checkpoint],\n",
        "        class_weight=CLASS_WEIGHTS\n",
        "    )\n",
        "\n",
        "    # Final evaluation\n",
        "    model = tf.keras.models.load_model('best_model.keras')\n",
        "    evaluate_model()\n"
      ],
      "metadata": {
        "id": "5O1gI8c56NxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa64d4ff-0a66-4bcf-86c7-5aefc0c0d58e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 52 images belonging to 2 classes.\n",
            "Found 12 images belonging to 2 classes.\n",
            "Class weights: {0: 0.5714285714285714, 1: 4.0}\n",
            "Class distribution:\n",
            "FAKE: 56 samples\n",
            "REAL: 8 samples\n",
            "\n",
            "Warning: Severe class imbalance detected!\n",
            "Consider data augmentation for minority class or collecting more samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 512ms/step - accuracy: 0.6068 - loss: 1.4684 - precision: 0.2330 - recall: 0.6500 - val_accuracy: 0.9167 - val_loss: 0.5955 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9375 - loss: 0.8720 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.5995 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/100\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.7500 - loss: 0.2178 - precision: 0.0000e+00 - recall: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 291ms/step - accuracy: 0.8431 - loss: 0.4799 - precision: 0.2917 - recall: 0.2500 - val_accuracy: 0.9167 - val_loss: 0.6107 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7500 - loss: 2.2624 - precision: 0.3333 - recall: 0.3333 - val_accuracy: 0.9167 - val_loss: 0.6224 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308ms/step - accuracy: 0.7917 - loss: 1.3544 - precision: 0.3622 - recall: 0.6250 - val_accuracy: 0.9167 - val_loss: 0.6604 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7500 - loss: 1.5412 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.6704 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 194ms/step - accuracy: 0.7717 - loss: 0.5652 - precision: 0.2125 - recall: 0.5417 - val_accuracy: 0.6667 - val_loss: 0.6853 - val_precision: 0.2000 - val_recall: 1.0000\n",
            "Epoch 8/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7500 - loss: 0.5373 - precision: 0.4286 - recall: 1.0000 - val_accuracy: 0.6667 - val_loss: 0.6870 - val_precision: 0.2000 - val_recall: 1.0000\n",
            "Epoch 9/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 181ms/step - accuracy: 0.6181 - loss: 1.5670 - precision: 0.0644 - recall: 0.2083 - val_accuracy: 0.3333 - val_loss: 0.6978 - val_precision: 0.1111 - val_recall: 1.0000\n",
            "Epoch 10/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5000 - loss: 1.7886 - precision: 0.1429 - recall: 0.3333 - val_accuracy: 0.1667 - val_loss: 0.7046 - val_precision: 0.0909 - val_recall: 1.0000\n",
            "Epoch 11/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 184ms/step - accuracy: 0.6927 - loss: 0.7129 - precision: 0.1493 - recall: 0.2875 - val_accuracy: 0.0833 - val_loss: 0.7318 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 12/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.4244 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.0833 - val_loss: 0.7435 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 13/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.6396 - loss: 0.4722 - precision: 0.1439 - recall: 0.7083 - val_accuracy: 0.0833 - val_loss: 0.7873 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 14/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8750 - loss: 1.7589 - precision: 0.7500 - recall: 0.7500 - val_accuracy: 0.0833 - val_loss: 0.8099 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 15/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 267ms/step - accuracy: 0.6432 - loss: 0.6736 - precision: 0.1854 - recall: 0.7083 - val_accuracy: 0.0833 - val_loss: 0.8869 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 16/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7500 - loss: 2.2817 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.0833 - val_loss: 0.9148 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 17/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 267ms/step - accuracy: 0.6797 - loss: 0.7670 - precision: 0.2205 - recall: 0.5357 - val_accuracy: 0.0833 - val_loss: 1.0213 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 18/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5000 - loss: 0.4465 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.0833 - val_loss: 1.0608 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 19/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 192ms/step - accuracy: 0.5993 - loss: 0.6303 - precision: 0.1572 - recall: 0.7500 - val_accuracy: 0.0833 - val_loss: 1.1915 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 20/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7500 - loss: 0.5528 - precision: 0.4286 - recall: 1.0000 - val_accuracy: 0.0833 - val_loss: 1.2405 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 21/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 281ms/step - accuracy: 0.8097 - loss: 0.6170 - precision: 0.5417 - recall: 0.9167 - val_accuracy: 0.0833 - val_loss: 1.3941 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 22/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7500 - loss: 0.4409 - precision: 0.2000 - recall: 1.0000 - val_accuracy: 0.0833 - val_loss: 1.4515 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 23/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 273ms/step - accuracy: 0.6927 - loss: 0.7023 - precision: 0.2125 - recall: 0.4643 - val_accuracy: 0.0833 - val_loss: 1.6263 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 24/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5000 - loss: 0.3850 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.0833 - val_loss: 1.6890 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 25/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 287ms/step - accuracy: 0.7111 - loss: 0.6997 - precision: 0.3239 - recall: 0.7167 - val_accuracy: 0.0833 - val_loss: 1.8738 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 26/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6250 - loss: 0.5790 - precision: 0.1667 - recall: 0.5000 - val_accuracy: 0.0833 - val_loss: 1.9437 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Epoch 27/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 290ms/step - accuracy: 0.7344 - loss: 0.4217 - precision: 0.2932 - recall: 1.0000 - val_accuracy: 0.0833 - val_loss: 2.1455 - val_precision: 0.0833 - val_recall: 1.0000\n",
            "Found 64 images belonging to 2 classes.\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.88      1.00      0.93        56\n",
            "        REAL       0.00      0.00      0.00         8\n",
            "\n",
            "    accuracy                           0.88        64\n",
            "   macro avg       0.44      0.50      0.47        64\n",
            "weighted avg       0.77      0.88      0.82        64\n",
            "\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9500 - loss: 0.5945 - precision: 0.0000e+00 - recall: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Audio Deepfake Detection using Transfer Learning\n",
        "\"\"\"\n",
        "\n",
        "# %% Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# %% Configuration\n",
        "SPECTROGRAM_PATH = r\"D:\\ML\\minor project\\archive\\KAGGLE\\SPECTROGRAMS\"\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 50\n",
        "CLASS_NAMES = ['FAKE', 'REAL']\n",
        "\n",
        "# %% Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    brightness_range=[0.8,1.2],\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='constant',\n",
        "    cval=0\n",
        ")\n",
        "\n",
        "# %% Data Generators\n",
        "def create_generator(datagen, subset):\n",
        "    return datagen.flow_from_directory(\n",
        "        SPECTROGRAM_PATH,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        subset=subset,\n",
        "        classes=CLASS_NAMES,\n",
        "        color_mode='rgb'  # Pretrained models need 3 channels\n",
        "    )\n",
        "\n",
        "train_generator = create_generator(train_datagen, 'training')\n",
        "val_generator = create_generator(train_datagen, 'validation')\n",
        "\n",
        "# %% Class Weight Calculation\n",
        "class_counts = train_generator.classes.sum(), len(train_generator.classes) - train_generator.classes.sum()\n",
        "total = sum(class_counts)\n",
        "CLASS_WEIGHTS = {0: total/(2*class_counts[0]), 1: total/(2*class_counts[1])}\n",
        "\n",
        "# %% Model Setup\n",
        "def create_transfer_model():\n",
        "    base_model = MobileNetV2(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(*IMG_SIZE, 3)\n",
        "    )\n",
        "\n",
        "    # Freeze base layers\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Add custom head\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = create_transfer_model()\n",
        "\n",
        "# %% Model Compilation\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "# %% Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_recall', patience=10, mode='max', restore_best_weights=True),\n",
        "    ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "]\n",
        "\n",
        "# %% Training\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=CLASS_WEIGHTS\n",
        ")\n",
        "\n",
        "# %% Evaluation\n",
        "def evaluate_model():\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        SPECTROGRAM_PATH,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        shuffle=False,\n",
        "        classes=CLASS_NAMES\n",
        "    )\n",
        "\n",
        "    # Load best model\n",
        "    model = tf.keras.models.load_model('best_model.h5')\n",
        "\n",
        "    # Generate predictions\n",
        "    y_true = test_generator.classes\n",
        "    y_pred = (model.predict(test_generator) > 0.5).astype(int)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.show()\n",
        "\n",
        "evaluate_model()\n",
        "\n",
        "# %% Prediction Function\n",
        "def predict_spectrogram(img_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(\n",
        "        img_path,\n",
        "        target_size=IMG_SIZE,\n",
        "        color_mode='rgb'\n",
        "    )\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "    prediction = model.predict(img_array)[0][0]\n",
        "    return \"FAKE\" if prediction > 0.5 else \"REAL\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iOX3MRGnIB9n",
        "outputId": "d31edf1f-e4ba-464e-8b53-0c800190bcfd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 52 images belonging to 2 classes.\n",
            "Found 12 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - accuracy: 0.2292 - loss: 4.3269 - precision_1: 0.1408 - recall_1: 0.9524"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_recall` which is not available. Available metrics are: accuracy,loss,precision_1,recall_1,val_accuracy,val_loss,val_precision_1,val_recall_1\n",
            "  current = self.get_monitor_value(logs)\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.2344 - loss: 4.2163 - precision_1: 0.1422 - recall_1: 0.9286 - val_accuracy: 0.1667 - val_loss: 0.9433 - val_precision_1: 0.0909 - val_recall_1: 1.0000\n",
            "Epoch 2/50\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0000e+00 - loss: 3.4770 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.0000e+00 - loss: 3.4770 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.0833 - val_loss: 0.9036 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 3/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.5907 - loss: 1.9393 - precision_1: 0.1508 - recall_1: 0.2333"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 526ms/step - accuracy: 0.6236 - loss: 1.9046 - precision_1: 0.1488 - recall_1: 0.2250 - val_accuracy: 0.8333 - val_loss: 0.4914 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 4/50\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8750 - loss: 1.1568 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 336ms/step - accuracy: 0.8750 - loss: 1.1568 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3717 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 5/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380ms/step - accuracy: 0.8618 - loss: 1.0673 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.2711 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 6/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - accuracy: 0.8750 - loss: 0.8100 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3022 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 7/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 498ms/step - accuracy: 0.8594 - loss: 0.6100 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3972 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 8/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.7500 - loss: 0.5862 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3425 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 9/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 476ms/step - accuracy: 0.8568 - loss: 0.4734 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.2553 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 10/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.1692 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.2917 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 11/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 497ms/step - accuracy: 0.8255 - loss: 0.4685 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3489 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 12/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.1125 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3834 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 13/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 363ms/step - accuracy: 0.8885 - loss: 0.3488 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3000 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 14/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9375 - loss: 0.2429 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3270 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 15/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 491ms/step - accuracy: 0.8306 - loss: 0.4643 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3804 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 16/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8750 - loss: 0.3408 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.5113 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 17/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358ms/step - accuracy: 0.8743 - loss: 0.3823 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3426 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 18/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy: 0.8750 - loss: 0.4092 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.4189 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 19/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 482ms/step - accuracy: 0.6903 - loss: 0.7632 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3709 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 20/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0684 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.4441 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 21/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 498ms/step - accuracy: 0.8620 - loss: 0.4051 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.4266 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 22/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.5000 - loss: 1.2459 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3242 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 23/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339ms/step - accuracy: 0.8446 - loss: 0.4395 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.2960 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 24/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy: 0.8750 - loss: 0.3062 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3963 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 25/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 483ms/step - accuracy: 0.9333 - loss: 0.2297 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3983 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 26/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.7500 - loss: 0.6758 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3127 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 27/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345ms/step - accuracy: 0.7934 - loss: 0.5817 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3777 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 28/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0475 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3138 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 29/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 357ms/step - accuracy: 0.8476 - loss: 0.4163 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3093 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 30/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.8125 - loss: 0.5101 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3744 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 31/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 473ms/step - accuracy: 0.8569 - loss: 0.4415 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.2992 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 32/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy: 0.8125 - loss: 0.5542 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3758 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 33/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 454ms/step - accuracy: 0.8359 - loss: 0.4232 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.2933 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 34/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.7500 - loss: 0.5669 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3894 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 35/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 360ms/step - accuracy: 0.8385 - loss: 0.4363 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3046 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 36/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - accuracy: 0.9375 - loss: 0.2066 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.2991 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 37/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 365ms/step - accuracy: 0.8168 - loss: 0.4790 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.2932 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 38/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.0917 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3208 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 39/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 483ms/step - accuracy: 0.8042 - loss: 0.5143 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3222 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 40/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - accuracy: 0.9375 - loss: 0.2365 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3577 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 41/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 467ms/step - accuracy: 0.8177 - loss: 0.4445 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3080 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 42/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0654 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3175 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 43/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 351ms/step - accuracy: 0.9038 - loss: 0.2577 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3051 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 44/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.8125 - loss: 0.4652 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.2986 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 45/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345ms/step - accuracy: 0.8681 - loss: 0.3796 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.2967 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 46/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - accuracy: 0.8750 - loss: 0.3726 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3423 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 47/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 356ms/step - accuracy: 0.8354 - loss: 0.4230 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.2739 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 48/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.9375 - loss: 0.2178 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3107 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 49/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 487ms/step - accuracy: 0.8568 - loss: 0.3755 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.2960 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 50/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0965 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.9167 - val_loss: 0.3464 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Found 64 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 196ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.87      0.96      0.92        56\n",
            "        REAL       0.00      0.00      0.00         8\n",
            "\n",
            "    accuracy                           0.84        64\n",
            "   macro avg       0.44      0.48      0.46        64\n",
            "weighted avg       0.76      0.84      0.80        64\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    }
  ]
}