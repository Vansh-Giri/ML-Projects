{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U2n_05WS-tw",
        "outputId": "cb13009f-6273-4aed-ec8d-9f3ec09eeda4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: D:\\ML\\minor project\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir(r'D:\\ML\\minor project')\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "print(librosa.__version__)  # Should be ≥0.11.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAKfOSlITGGT",
        "outputId": "fe84f996-a461-4570-eab8-3bcf25eaf5a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch\n",
        "\n",
        "# Configuration\n",
        "real_dir = r\"D:\\ML\\minor project\\archive\\KAGGLE\\AUDIO\\REAL\"\n",
        "output_dir = r\"D:\\ML\\minor project\\archive\\KAGGLE\\SPECTROGRAMS\\REAL_GEN\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Target dimensions and parameters\n",
        "target_size = (173, 172)\n",
        "sr = 22050\n",
        "n_mels = 128\n",
        "aug_per_file = 7\n",
        "\n",
        "# Augmentation pipeline\n",
        "augment = Compose([\n",
        "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
        "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
        "    TimeStretch(min_rate=0.8, max_rate=1.2, p=0.5),\n",
        "])\n",
        "\n",
        "def create_spectrogram(y, sr, save_path):\n",
        "    \"\"\"Convert audio to Mel-spectrogram matching original color scheme and size\"\"\"\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    # Calculate precise figure size to get 173x172 output\n",
        "    fig = plt.figure(figsize=(target_size[0]/100, target_size[1]/100), dpi=100, frameon=False)\n",
        "\n",
        "    # Match exact colormap from original\n",
        "    librosa.display.specshow(S_dB, sr=sr, cmap='plasma')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0, dpi=100)\n",
        "    plt.close()\n",
        "\n",
        "    # Verify dimensions\n",
        "    from PIL import Image\n",
        "    img = Image.open(save_path)\n",
        "    if img.size != target_size:\n",
        "        img = img.resize(target_size)\n",
        "        img.save(save_path)\n",
        "\n",
        "# Only augmented files counter\n",
        "generated_count = 0\n",
        "\n",
        "# Process each original file\n",
        "for file in os.listdir(real_dir):\n",
        "    if file.lower().endswith('.wav'):\n",
        "        base_name = os.path.splitext(file)[0]\n",
        "        file_path = os.path.join(real_dir, file)\n",
        "\n",
        "        # Load audio with soundfile\n",
        "        y, orig_sr = sf.read(file_path)\n",
        "        if y.ndim > 1:\n",
        "            y = y.mean(axis=1)\n",
        "        if orig_sr != sr:\n",
        "            y = librosa.resample(y, orig_sr=orig_sr, target_sr=sr)\n",
        "\n",
        "        # Generate exactly 7 augmentations per file (no originals)\n",
        "        for i in range(aug_per_file):\n",
        "            y_aug = augment(samples=y, sample_rate=sr)\n",
        "            output_path = os.path.join(output_dir, f\"{base_name}_aug_{i}.png\")\n",
        "            create_spectrogram(y_aug, sr, output_path)\n",
        "            generated_count += 1\n",
        "\n",
        "print(f\"Generated exactly {generated_count} spectrograms (173x172px) in {output_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aP74u5RTPfe",
        "outputId": "0a4d1bc3-ee0f-43d0-e8f6-615990dd52c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated exactly 56 spectrograms (173x172px) in D:\\ML\\minor project\\archive\\KAGGLE\\SPECTROGRAMS\\REAL_GEN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Enhanced Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten,\n",
        "                                   Dense, Dropout, BatchNormalization)\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import seaborn as sns\n",
        "\n",
        "# %% Configuration\n",
        "SPECTROGRAM_PATH = r\"D:\\ML\\minor project\\archive\\KAGGLE\\SPECTROGRAMS\"\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 8  # Reduced for better batch coverage\n",
        "EPOCHS = 100\n",
        "CLASS_NAMES = ['FAKE', 'REAL']\n",
        "VAL_SPLIT = 0.2\n",
        "\n",
        "# %% Enhanced Data Pipeline\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=VAL_SPLIT,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='constant',\n",
        "    cval=0\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    SPECTROGRAM_PATH,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    classes=CLASS_NAMES,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    SPECTROGRAM_PATH,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    classes=CLASS_NAMES,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# %% Proper Class Weight Calculation\n",
        "def calculate_class_weights():\n",
        "    class_counts = np.bincount(train_generator.classes)\n",
        "    return {\n",
        "        0: class_counts[1]/class_counts[0],  # FAKE weight\n",
        "        1: class_counts[0]/class_counts[1]   # REAL weight\n",
        "    }\n",
        "\n",
        "CLASS_WEIGHTS = calculate_class_weights()\n",
        "print(f\"Effective class weights: {CLASS_WEIGHTS}\")\n",
        "\n",
        "# %% Simplified Model Architecture\n",
        "def create_cnn_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(16, (3,3), activation='relu', input_shape=(*IMG_SIZE, 3)),\n",
        "        MaxPooling2D(2,2),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Conv2D(32, (3,3), activation='relu'),\n",
        "        MaxPooling2D(2,2),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-4),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# %% Training Configuration\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=30,\n",
        "    min_delta=0.01,\n",
        "    mode='max',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max'\n",
        ")\n",
        "\n",
        "# %% Enhanced Evaluation with Path Verification\n",
        "def evaluate_model():\n",
        "    y_true = val_generator.classes\n",
        "    y_pred_probs = model.predict(val_generator)\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=CLASS_NAMES, zero_division=0))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(confusion_matrix(y_true, y_pred),\n",
        "                annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(os.path.abspath('confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_probs)\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig(os.path.abspath('roc_curve.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"\\nVisualizations saved to:\")\n",
        "    print(f\"- {os.path.abspath('confusion_matrix.png')}\")\n",
        "    print(f\"- {os.path.abspath('roc_curve.png')}\")\n",
        "\n",
        "    return model.evaluate(val_generator)\n",
        "\n",
        "# %% Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Verify dataset\n",
        "    print(f\"Training samples: {train_generator.samples}\")\n",
        "    print(f\"Validation samples: {val_generator.samples}\")\n",
        "\n",
        "    # Model initialization\n",
        "    model = create_cnn_model()\n",
        "\n",
        "    # Calculate steps with ceiling\n",
        "    steps_per_epoch = np.ceil(train_generator.samples / BATCH_SIZE).astype(int)\n",
        "    val_steps = np.ceil(val_generator.samples / BATCH_SIZE).astype(int)\n",
        "\n",
        "    # Training\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_generator,\n",
        "        validation_steps=val_steps,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[early_stop, checkpoint],\n",
        "        class_weight=CLASS_WEIGHTS,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Final evaluation\n",
        "    model = tf.keras.models.load_model('best_model.keras')\n",
        "    evaluate_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4IFSM_eWzcL",
        "outputId": "bab1cbdb-bd79-4c9c-f895-9d609d701876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 90 images belonging to 2 classes.\n",
            "Found 22 images belonging to 2 classes.\n",
            "Effective class weights: {0: 1.0, 1: 1.0}\n",
            "Training samples: 90\n",
            "Validation samples: 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 242ms/step - accuracy: 0.7865 - loss: 0.7273 - precision: 0.7366 - recall: 0.8652 - val_accuracy: 0.5000 - val_loss: 0.9594 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 2/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 159ms/step - accuracy: 0.9353 - loss: 0.1866 - precision: 0.9425 - recall: 0.9243 - val_accuracy: 0.5000 - val_loss: 0.8649 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 3/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 159ms/step - accuracy: 0.9871 - loss: 0.0474 - precision: 0.9860 - recall: 0.9884 - val_accuracy: 0.5000 - val_loss: 1.0052 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 4/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 176ms/step - accuracy: 0.9453 - loss: 0.2208 - precision: 0.9682 - recall: 0.9333 - val_accuracy: 0.5000 - val_loss: 1.1734 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 5/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 166ms/step - accuracy: 0.9571 - loss: 0.1942 - precision: 0.9497 - recall: 0.9757 - val_accuracy: 0.5000 - val_loss: 1.1186 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 6/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 155ms/step - accuracy: 0.9908 - loss: 0.0495 - precision: 1.0000 - recall: 0.9817 - val_accuracy: 0.5000 - val_loss: 0.7408 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 7/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 161ms/step - accuracy: 0.9249 - loss: 0.2811 - precision: 0.9760 - recall: 0.8736 - val_accuracy: 0.5000 - val_loss: 0.9119 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 8/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 188ms/step - accuracy: 0.9576 - loss: 0.1283 - precision: 0.9312 - recall: 0.9877 - val_accuracy: 0.5455 - val_loss: 0.7431 - val_precision: 0.5238 - val_recall: 1.0000\n",
            "Epoch 9/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 186ms/step - accuracy: 1.0000 - loss: 0.0032 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.5909 - val_loss: 0.5545 - val_precision: 0.5500 - val_recall: 1.0000\n",
            "Epoch 10/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 185ms/step - accuracy: 1.0000 - loss: 3.0733e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.7273 - val_loss: 0.5095 - val_precision: 0.6471 - val_recall: 1.0000\n",
            "Epoch 11/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step - accuracy: 0.9109 - loss: 0.5100 - precision: 0.8087 - recall: 0.8570 - val_accuracy: 0.5000 - val_loss: 1.0736 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 12/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 151ms/step - accuracy: 0.9696 - loss: 0.1089 - precision: 0.9652 - recall: 0.9718 - val_accuracy: 0.5000 - val_loss: 1.5807 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 13/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.0014 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.5000 - val_loss: 1.6567 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 14/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 151ms/step - accuracy: 0.9905 - loss: 0.0148 - precision: 1.0000 - recall: 0.9806 - val_accuracy: 0.5000 - val_loss: 1.6384 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 15/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step - accuracy: 0.9852 - loss: 0.0451 - precision: 0.9702 - recall: 1.0000 - val_accuracy: 0.5000 - val_loss: 0.8582 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 16/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 158ms/step - accuracy: 0.9966 - loss: 0.0129 - precision: 1.0000 - recall: 0.9932 - val_accuracy: 0.5000 - val_loss: 0.7786 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 17/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0098 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.5000 - val_loss: 1.0566 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 18/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0109 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.5455 - val_loss: 0.9736 - val_precision: 0.5238 - val_recall: 1.0000\n",
            "Epoch 19/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0051 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.5455 - val_loss: 1.1662 - val_precision: 0.5238 - val_recall: 1.0000\n",
            "Epoch 20/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 2.6563e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.5000 - val_loss: 0.8150 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 21/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 3.7488e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.5909 - val_loss: 0.9627 - val_precision: 0.5500 - val_recall: 1.0000\n",
            "Epoch 22/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 156ms/step - accuracy: 0.9951 - loss: 0.0128 - precision: 1.0000 - recall: 0.9904 - val_accuracy: 0.5000 - val_loss: 1.5482 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 23/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 152ms/step - accuracy: 0.9833 - loss: 0.1348 - precision: 0.9695 - recall: 1.0000 - val_accuracy: 0.5000 - val_loss: 2.7243 - val_precision: 0.5000 - val_recall: 1.0000\n",
            "Epoch 24/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 184ms/step - accuracy: 0.9366 - loss: 0.1817 - precision: 0.8802 - recall: 1.0000 - val_accuracy: 0.9545 - val_loss: 0.0672 - val_precision: 0.9167 - val_recall: 1.0000\n",
            "Epoch 25/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step - accuracy: 0.9575 - loss: 0.1945 - precision: 1.0000 - recall: 0.9118 - val_accuracy: 0.7273 - val_loss: 0.9155 - val_precision: 0.6471 - val_recall: 1.0000\n",
            "Epoch 26/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step - accuracy: 0.9852 - loss: 0.0204 - precision: 0.9966 - recall: 0.9729 - val_accuracy: 0.7273 - val_loss: 0.6921 - val_precision: 0.6471 - val_recall: 1.0000\n",
            "Epoch 27/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 177ms/step - accuracy: 0.9793 - loss: 0.1195 - precision: 0.9947 - recall: 0.9654 - val_accuracy: 1.0000 - val_loss: 0.0161 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 28/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 156ms/step - accuracy: 0.9938 - loss: 0.0363 - precision: 1.0000 - recall: 0.9881 - val_accuracy: 0.9545 - val_loss: 0.0419 - val_precision: 0.9167 - val_recall: 1.0000\n",
            "Epoch 29/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0012 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 0.0148 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 30/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 2.6389e-05 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 0.0137 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 31/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 6.9479e-06 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 0.0103 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 32/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 4.5781e-06 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 0.0121 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 33/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 168ms/step - accuracy: 0.9905 - loss: 0.0099 - precision: 0.9804 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 0.0047 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 34/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 170ms/step - accuracy: 0.9974 - loss: 0.0051 - precision: 0.9948 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 0.0011 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 35/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 157ms/step - accuracy: 0.9682 - loss: 0.1979 - precision: 0.9028 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 5.7019e-05 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 36/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 147ms/step - accuracy: 0.9690 - loss: 0.1358 - precision: 1.0000 - recall: 0.9386 - val_accuracy: 1.0000 - val_loss: 2.2895e-05 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 37/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.0040 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 0.0203 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 38/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.0026 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 0.0080 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 39/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 9.9679e-07 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 0.0012 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 40/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 1.1513e-05 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 0.0252 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 41/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 149ms/step - accuracy: 0.9983 - loss: 0.0083 - precision: 0.9967 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 9.0960e-06 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 42/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 4.7479e-05 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 2.2225e-07 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 43/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 152ms/step - accuracy: 0.9963 - loss: 0.0172 - precision: 1.0000 - recall: 0.9927 - val_accuracy: 1.0000 - val_loss: 3.0850e-05 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 44/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 149ms/step - accuracy: 0.9705 - loss: 0.0722 - precision: 0.9345 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 4.8221e-09 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 45/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 2.9258e-06 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 8.8343e-07 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 46/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 1.8389e-05 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 6.5504e-07 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 47/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 158ms/step - accuracy: 0.9619 - loss: 0.0808 - precision: 1.0000 - recall: 0.9312 - val_accuracy: 1.0000 - val_loss: 4.2131e-09 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 48/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 4.3858e-06 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 4.0466e-09 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 49/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 149ms/step - accuracy: 0.9837 - loss: 0.0261 - precision: 0.9688 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 2.5734e-04 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 50/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 147ms/step - accuracy: 0.9951 - loss: 0.0267 - precision: 1.0000 - recall: 0.9902 - val_accuracy: 1.0000 - val_loss: 7.1928e-05 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 51/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 1.6699e-06 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 2.4725e-09 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 52/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 152ms/step - accuracy: 0.9890 - loss: 0.2071 - precision: 0.9902 - recall: 0.9877 - val_accuracy: 1.0000 - val_loss: 2.7553e-08 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 53/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 155ms/step - accuracy: 0.9690 - loss: 0.0557 - precision: 0.9388 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 2.3525e-10 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 54/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.0096 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 2.6240e-08 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 55/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 6.5871e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 1.4219e-06 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 56/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 9.0650e-08 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 2.2227e-10 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 57/100\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 147ms/step - accuracy: 0.9852 - loss: 0.0218 - precision: 0.9718 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 2.3224e-06 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002A98FB058A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002A98FB058A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       1.00      1.00      1.00        11\n",
            "        REAL       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        22\n",
            "   macro avg       1.00      1.00      1.00        22\n",
            "weighted avg       1.00      1.00      1.00        22\n",
            "\n",
            "\n",
            "Visualizations saved to:\n",
            "- D:\\ML\\minor project\\confusion_matrix.png\n",
            "- D:\\ML\\minor project\\roc_curve.png\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0349 - precision: 0.7500 - recall: 0.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Audio Deepfake Detection using Transfer Learning\n",
        "\"\"\"\n",
        "\n",
        "# %% Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# %% Configuration\n",
        "SPECTROGRAM_PATH = r\"D:\\ML\\minor project\\archive\\KAGGLE\\SPECTROGRAMS\"\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 50\n",
        "CLASS_NAMES = ['FAKE', 'REAL']\n",
        "\n",
        "# %% Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    brightness_range=[0.8,1.2],\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='constant',\n",
        "    cval=0\n",
        ")\n",
        "\n",
        "# %% Data Generators\n",
        "def create_generator(datagen, subset):\n",
        "    return datagen.flow_from_directory(\n",
        "        SPECTROGRAM_PATH,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        subset=subset,\n",
        "        classes=CLASS_NAMES,\n",
        "        color_mode='rgb'  # Pretrained models need 3 channels\n",
        "    )\n",
        "\n",
        "train_generator = create_generator(train_datagen, 'training')\n",
        "val_generator = create_generator(train_datagen, 'validation')\n",
        "\n",
        "# %% Class Weight Calculation\n",
        "class_counts = train_generator.classes.sum(), len(train_generator.classes) - train_generator.classes.sum()\n",
        "total = sum(class_counts)\n",
        "CLASS_WEIGHTS = {0: total/(2*class_counts[0]), 1: total/(2*class_counts[1])}\n",
        "\n",
        "# %% Model Setup\n",
        "def create_transfer_model():\n",
        "    base_model = MobileNetV2(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(*IMG_SIZE, 3)\n",
        "    )\n",
        "\n",
        "    # Freeze base layers\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Add custom head\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = create_transfer_model()\n",
        "\n",
        "# %% Model Compilation\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "# %% Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_recall', patience=10, mode='max', restore_best_weights=True),\n",
        "    ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "]\n",
        "\n",
        "# %% Training\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=CLASS_WEIGHTS\n",
        ")\n",
        "\n",
        "# %% Evaluation\n",
        "def evaluate_model():\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        SPECTROGRAM_PATH,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        shuffle=False,\n",
        "        classes=CLASS_NAMES\n",
        "    )\n",
        "\n",
        "    # Load best model\n",
        "    model = tf.keras.models.load_model('best_model.h5')\n",
        "\n",
        "    # Generate predictions\n",
        "    y_true = test_generator.classes\n",
        "    y_pred = (model.predict(test_generator) > 0.5).astype(int)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.show()\n",
        "\n",
        "evaluate_model()\n",
        "\n",
        "# %% Prediction Function\n",
        "def predict_spectrogram(img_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(\n",
        "        img_path,\n",
        "        target_size=IMG_SIZE,\n",
        "        color_mode='rgb'\n",
        "    )\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "    prediction = model.predict(img_array)[0][0]\n",
        "    return \"FAKE\" if prediction > 0.5 else \"REAL\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "csOSBOcQZl4h",
        "outputId": "947b82b1-1abc-4fb4-aaeb-a7dd2cfcc4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 90 images belonging to 2 classes.\n",
            "Found 22 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.6249 - loss: 0.6436 - precision: 0.6412 - recall: 0.5981"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.6266 - loss: 0.6439 - precision: 0.6424 - recall: 0.6037 - val_accuracy: 0.5625 - val_loss: 0.6702 - val_precision: 0.5000 - val_recall: 0.7143\n",
            "Epoch 2/50\n",
            "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8750 - loss: 0.5837 - precision: 0.7778 - recall: 1.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 206ms/step - accuracy: 0.8750 - loss: 0.5837 - precision: 0.7778 - recall: 1.0000 - val_accuracy: 0.6875 - val_loss: 0.6844 - val_precision: 0.7000 - val_recall: 0.7778\n",
            "Epoch 3/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - accuracy: 0.7216 - loss: 0.5665 - precision: 0.7677 - recall: 0.6631"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 509ms/step - accuracy: 0.7275 - loss: 0.5623 - precision: 0.7721 - recall: 0.6710 - val_accuracy: 0.8125 - val_loss: 0.5201 - val_precision: 0.7500 - val_recall: 0.8571\n",
            "Epoch 4/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 0.7500 - loss: 0.5065 - precision: 0.8000 - recall: 0.5714 - val_accuracy: 0.8125 - val_loss: 0.5521 - val_precision: 0.8571 - val_recall: 0.7500\n",
            "Epoch 5/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 0.8722 - loss: 0.4657 - precision: 0.9251 - recall: 0.7867"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 473ms/step - accuracy: 0.8710 - loss: 0.4653 - precision: 0.9314 - recall: 0.7794 - val_accuracy: 0.8750 - val_loss: 0.4163 - val_precision: 1.0000 - val_recall: 0.7500\n",
            "Epoch 6/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 0.9375 - loss: 0.4347 - precision: 0.9091 - recall: 1.0000 - val_accuracy: 0.7500 - val_loss: 0.4566 - val_precision: 0.7143 - val_recall: 0.7143\n",
            "Epoch 7/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 412ms/step - accuracy: 0.9436 - loss: 0.3431 - precision: 0.9764 - recall: 0.9043 - val_accuracy: 0.8750 - val_loss: 0.4380 - val_precision: 1.0000 - val_recall: 0.7500\n",
            "Epoch 8/50\n",
            "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.7000 - loss: 0.6275 - precision: 1.0000 - recall: 0.5714"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 181ms/step - accuracy: 0.7000 - loss: 0.6275 - precision: 1.0000 - recall: 0.5714 - val_accuracy: 1.0000 - val_loss: 0.3082 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 9/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 426ms/step - accuracy: 0.9389 - loss: 0.3063 - precision: 1.0000 - recall: 0.8759 - val_accuracy: 1.0000 - val_loss: 0.3270 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 10/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step - accuracy: 0.8750 - loss: 0.3139 - precision: 1.0000 - recall: 0.8333 - val_accuracy: 0.8750 - val_loss: 0.3309 - val_precision: 0.8889 - val_recall: 0.8889\n",
            "Epoch 11/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 412ms/step - accuracy: 0.8800 - loss: 0.3039 - precision: 0.9567 - recall: 0.8120 - val_accuracy: 1.0000 - val_loss: 0.2344 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 12/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 0.1864 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9375 - val_loss: 0.2763 - val_precision: 1.0000 - val_recall: 0.8889\n",
            "Epoch 13/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385ms/step - accuracy: 0.9557 - loss: 0.2673 - precision: 1.0000 - recall: 0.9133 - val_accuracy: 1.0000 - val_loss: 0.1577 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 14/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 0.1814 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 0.1753 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 15/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 415ms/step - accuracy: 0.9312 - loss: 0.2415 - precision: 1.0000 - recall: 0.8212 - val_accuracy: 1.0000 - val_loss: 0.1852 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 16/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 1.0000 - loss: 0.2455 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 0.2005 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 17/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380ms/step - accuracy: 0.9761 - loss: 0.2023 - precision: 0.9847 - recall: 0.9653 - val_accuracy: 1.0000 - val_loss: 0.1659 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 18/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step - accuracy: 0.8750 - loss: 0.2725 - precision: 0.9000 - recall: 0.9000 - val_accuracy: 0.9375 - val_loss: 0.1645 - val_precision: 0.8571 - val_recall: 1.0000\n",
            "Found 112 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 196ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.75      1.00      0.85        56\n",
            "        REAL       1.00      0.66      0.80        56\n",
            "\n",
            "    accuracy                           0.83       112\n",
            "   macro avg       0.87      0.83      0.83       112\n",
            "weighted avg       0.87      0.83      0.83       112\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
        "\n",
        "# Configuration\n",
        "DATA_DIR = r\"D:\\ML\\minor project\\archive\\KAGGLE\\SPECTROGRAMS\"\n",
        "IMG_SIZE = (173, 172)\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 100\n",
        "CLASS_NAMES = ['FAKE', 'REAL']\n",
        "VAL_SPLIT = 0.2\n",
        "\n",
        "# Data Pipeline\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=VAL_SPLIT,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='constant'\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    DATA_DIR,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    classes=CLASS_NAMES,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    DATA_DIR,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    classes=CLASS_NAMES,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Model Architecture\n",
        "def create_model():\n",
        "    base_model = MobileNetV2(\n",
        "        input_shape=(*IMG_SIZE, 3),\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    # Freeze initial layers\n",
        "    for layer in base_model.layers[:-4]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-5),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy',\n",
        "                tf.keras.metrics.Precision(name='precision'),\n",
        "                tf.keras.metrics.Recall(name='recall')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Training Configuration\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "    ModelCheckpoint('best_spectrogram_model.keras', save_best_only=True)\n",
        "]\n",
        "\n",
        "# Model Training\n",
        "model = create_model()\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Evaluation and Visualization\n",
        "def generate_evaluation():\n",
        "    # Generate predictions\n",
        "    y_true = val_generator.classes\n",
        "    y_pred_probs = model.predict(val_generator)\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(confusion_matrix(y_true, y_pred),\n",
        "                annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_probs)\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "             label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Training History\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"\\nEvaluation visualizations saved:\")\n",
        "    print(\"- confusion_matrix.png\")\n",
        "    print(\"- roc_curve.png\")\n",
        "    print(\"- training_history.png\")\n",
        "\n",
        "generate_evaluation()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SAc4wm7iCH0",
        "outputId": "67cd93db-f7bd-418c-9d64-68eefe2d7b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 90 images belonging to 2 classes.\n",
            "Found 22 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_21340\\2105770840.py:56: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 406ms/step - accuracy: 0.4749 - loss: 1.0200 - precision: 0.3315 - recall: 0.0888 - val_accuracy: 0.6875 - val_loss: 0.5625 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/100\n",
            "\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6250 - loss: 0.7749 - precision: 1.0000 - recall: 0.2500"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.6250 - loss: 0.7749 - precision: 1.0000 - recall: 0.2500 - val_accuracy: 0.6875 - val_loss: 0.5294 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - accuracy: 0.6083 - loss: 0.7698 - precision: 0.4321 - recall: 0.2228 - val_accuracy: 0.6875 - val_loss: 0.5729 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.5485 - precision: 1.0000 - recall: 0.3333 - val_accuracy: 0.6875 - val_loss: 0.6127 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 176ms/step - accuracy: 0.5130 - loss: 0.8303 - precision: 0.7037 - recall: 0.2469 - val_accuracy: 0.7500 - val_loss: 0.4833 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 6/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6250 - loss: 0.6372 - precision: 1.0000 - recall: 0.4000 - val_accuracy: 0.6875 - val_loss: 0.6023 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - accuracy: 0.6095 - loss: 0.7379 - precision: 0.8519 - recall: 0.2770 - val_accuracy: 0.7500 - val_loss: 0.4944 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 8/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.3750 - loss: 0.8241 - precision: 1.0000 - recall: 0.1667 - val_accuracy: 0.6250 - val_loss: 0.4802 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step - accuracy: 0.6080 - loss: 0.7186 - precision: 0.7664 - recall: 0.3118 - val_accuracy: 0.6875 - val_loss: 0.4922 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8750 - loss: 0.4353 - precision: 1.0000 - recall: 0.6667 - val_accuracy: 0.6875 - val_loss: 0.5253 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - accuracy: 0.6922 - loss: 0.6160 - precision: 0.8702 - recall: 0.4701 - val_accuracy: 0.6875 - val_loss: 0.5507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 12/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.6250 - loss: 0.5326 - precision: 0.5000 - recall: 0.3333 - val_accuracy: 0.7500 - val_loss: 0.4548 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 13/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 154ms/step - accuracy: 0.6458 - loss: 0.5730 - precision: 0.8236 - recall: 0.3923 - val_accuracy: 0.7500 - val_loss: 0.4372 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 14/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6250 - loss: 0.7886 - precision: 1.0000 - recall: 0.2500 - val_accuracy: 0.7500 - val_loss: 0.4525 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 15/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 162ms/step - accuracy: 0.7233 - loss: 0.5078 - precision: 0.8717 - recall: 0.5303 - val_accuracy: 0.8125 - val_loss: 0.3944 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 16/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.7500 - loss: 0.7157 - precision: 1.0000 - recall: 0.3333 - val_accuracy: 0.8125 - val_loss: 0.3677 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 17/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - accuracy: 0.7013 - loss: 0.5560 - precision: 0.9020 - recall: 0.4868 - val_accuracy: 0.7500 - val_loss: 0.4781 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 18/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1902 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.7500 - val_loss: 0.4890 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 19/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - accuracy: 0.8062 - loss: 0.4755 - precision: 0.9040 - recall: 0.6553 - val_accuracy: 0.6875 - val_loss: 0.4220 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 20/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.2820 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.7500 - val_loss: 0.3757 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 21/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 173ms/step - accuracy: 0.6221 - loss: 0.6498 - precision: 0.8413 - recall: 0.3060 - val_accuracy: 0.7500 - val_loss: 0.3487 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 22/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6250 - loss: 0.6425 - precision: 1.0000 - recall: 0.2500 - val_accuracy: 0.7500 - val_loss: 0.4265 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 23/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - accuracy: 0.7785 - loss: 0.4253 - precision: 0.9851 - recall: 0.5930 - val_accuracy: 0.7500 - val_loss: 0.3938 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 24/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8750 - loss: 0.3601 - precision: 1.0000 - recall: 0.6667 - val_accuracy: 0.8750 - val_loss: 0.3384 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 25/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - accuracy: 0.7899 - loss: 0.4323 - precision: 0.8286 - recall: 0.7205 - val_accuracy: 0.7500 - val_loss: 0.4453 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 26/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.3435 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.8125 - val_loss: 0.3860 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 27/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.7889 - loss: 0.3990 - precision: 0.8630 - recall: 0.6626 - val_accuracy: 0.7500 - val_loss: 0.3954 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 28/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8750 - loss: 0.3658 - precision: 1.0000 - recall: 0.6667 - val_accuracy: 0.7500 - val_loss: 0.3407 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 29/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.8245 - loss: 0.3679 - precision: 0.8915 - recall: 0.7400 - val_accuracy: 0.7500 - val_loss: 0.3643 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 30/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8750 - loss: 0.4343 - precision: 1.0000 - recall: 0.7500 - val_accuracy: 0.8125 - val_loss: 0.3212 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 31/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 168ms/step - accuracy: 0.7774 - loss: 0.4364 - precision: 0.9167 - recall: 0.5678 - val_accuracy: 0.8125 - val_loss: 0.3117 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 32/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8750 - loss: 0.4592 - precision: 1.0000 - recall: 0.6667 - val_accuracy: 0.9375 - val_loss: 0.2526 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 33/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - accuracy: 0.8622 - loss: 0.3690 - precision: 0.9851 - recall: 0.7416 - val_accuracy: 0.7500 - val_loss: 0.4085 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 34/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.6250 - loss: 0.7060 - precision: 1.0000 - recall: 0.5000 - val_accuracy: 0.8125 - val_loss: 0.3425 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 35/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.7573 - loss: 0.4178 - precision: 0.8988 - recall: 0.6221 - val_accuracy: 0.8125 - val_loss: 0.3537 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 36/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8750 - loss: 0.2026 - precision: 1.0000 - recall: 0.7500 - val_accuracy: 0.8750 - val_loss: 0.2763 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 37/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 0.8348 - loss: 0.4108 - precision: 0.9946 - recall: 0.6899 - val_accuracy: 0.8125 - val_loss: 0.2995 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 38/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.2042 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9375 - val_loss: 0.2917 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 39/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - accuracy: 0.8441 - loss: 0.3527 - precision: 0.9277 - recall: 0.7721 - val_accuracy: 0.7500 - val_loss: 0.3680 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 40/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8750 - loss: 0.2812 - precision: 0.6667 - recall: 1.0000 - val_accuracy: 0.8750 - val_loss: 0.2391 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 41/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - accuracy: 0.8441 - loss: 0.4024 - precision: 0.7700 - recall: 0.8354 - val_accuracy: 0.8750 - val_loss: 0.2451 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 42/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.4377 - precision: 1.0000 - recall: 0.6000 - val_accuracy: 0.8750 - val_loss: 0.3272 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 43/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - accuracy: 0.8431 - loss: 0.4400 - precision: 0.9449 - recall: 0.7383 - val_accuracy: 0.8125 - val_loss: 0.3213 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 44/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8750 - loss: 0.2695 - precision: 1.0000 - recall: 0.6667 - val_accuracy: 0.7500 - val_loss: 0.4165 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 45/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - accuracy: 0.8059 - loss: 0.3537 - precision: 0.8261 - recall: 0.6905 - val_accuracy: 0.8125 - val_loss: 0.3385 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 46/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.1574 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.8750 - val_loss: 0.2368 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 47/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - accuracy: 0.8430 - loss: 0.3636 - precision: 0.8787 - recall: 0.7836 - val_accuracy: 0.8125 - val_loss: 0.3112 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 48/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8750 - loss: 0.4025 - precision: 1.0000 - recall: 0.8000 - val_accuracy: 0.9375 - val_loss: 0.2731 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 49/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - accuracy: 0.8824 - loss: 0.3061 - precision: 0.9559 - recall: 0.7939 - val_accuracy: 0.8750 - val_loss: 0.3597 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 50/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8750 - loss: 0.2894 - precision: 1.0000 - recall: 0.6667 - val_accuracy: 0.8750 - val_loss: 0.2485 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 51/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.7980 - loss: 0.3895 - precision: 0.9196 - recall: 0.6592 - val_accuracy: 0.8125 - val_loss: 0.3491 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 52/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.7500 - loss: 0.3942 - precision: 0.3333 - recall: 1.0000 - val_accuracy: 0.8750 - val_loss: 0.2303 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 53/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.7646 - loss: 0.5081 - precision: 0.8959 - recall: 0.6445 - val_accuracy: 0.8750 - val_loss: 0.2709 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 54/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8750 - loss: 0.6119 - precision: 1.0000 - recall: 0.8000 - val_accuracy: 0.8125 - val_loss: 0.2823 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 55/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.9339 - loss: 0.2297 - precision: 0.9538 - recall: 0.9038 - val_accuracy: 0.8750 - val_loss: 0.2536 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 56/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.6632 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.8750 - val_loss: 0.2412 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 57/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - accuracy: 0.8707 - loss: 0.2858 - precision: 0.9053 - recall: 0.8305 - val_accuracy: 0.8125 - val_loss: 0.2908 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 58/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8750 - loss: 0.2923 - precision: 1.0000 - recall: 0.7500 - val_accuracy: 0.8125 - val_loss: 0.2977 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 59/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 114ms/step - accuracy: 0.8639 - loss: 0.3216 - precision: 0.8657 - recall: 0.7993 - val_accuracy: 0.7500 - val_loss: 0.3743 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 60/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8750 - loss: 0.2484 - precision: 1.0000 - recall: 0.7500 - val_accuracy: 1.0000 - val_loss: 0.1834 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 61/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - accuracy: 0.9069 - loss: 0.2181 - precision: 0.9958 - recall: 0.8242 - val_accuracy: 0.8750 - val_loss: 0.2325 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 62/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.5029 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.9375 - val_loss: 0.2058 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 63/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.9260 - loss: 0.2732 - precision: 0.8922 - recall: 0.9587 - val_accuracy: 0.7500 - val_loss: 0.2565 - val_precision: 1.0000 - val_recall: 0.2000\n",
            "Epoch 64/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1776 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.8750 - val_loss: 0.2973 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 65/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - accuracy: 0.9141 - loss: 0.2320 - precision: 0.9580 - recall: 0.8563 - val_accuracy: 0.9375 - val_loss: 0.2096 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 66/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.1662 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 0.1564 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 67/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - accuracy: 0.9504 - loss: 0.2226 - precision: 1.0000 - recall: 0.9026 - val_accuracy: 0.9375 - val_loss: 0.1781 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 68/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8750 - loss: 0.3501 - precision: 1.0000 - recall: 0.8000 - val_accuracy: 0.8750 - val_loss: 0.2472 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 69/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - accuracy: 0.8612 - loss: 0.3276 - precision: 0.9137 - recall: 0.7989 - val_accuracy: 0.9375 - val_loss: 0.1718 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 70/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.2387 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.8125 - val_loss: 0.2623 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 71/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - accuracy: 0.9328 - loss: 0.2109 - precision: 0.9581 - recall: 0.8999 - val_accuracy: 1.0000 - val_loss: 0.2025 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 72/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.1294 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.8750 - val_loss: 0.3332 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 73/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - accuracy: 0.7996 - loss: 0.3364 - precision: 0.7449 - recall: 0.7210 - val_accuracy: 0.9375 - val_loss: 0.1731 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 74/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 0.8750 - loss: 0.1816 - precision: 1.0000 - recall: 0.8333 - val_accuracy: 1.0000 - val_loss: 0.1260 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 75/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - accuracy: 0.9382 - loss: 0.2874 - precision: 0.9632 - recall: 0.9121 - val_accuracy: 0.9375 - val_loss: 0.1898 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 76/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.1237 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9375 - val_loss: 0.1817 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 77/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - accuracy: 0.9084 - loss: 0.3064 - precision: 0.9757 - recall: 0.8304 - val_accuracy: 0.9375 - val_loss: 0.2476 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 78/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1496 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9375 - val_loss: 0.1756 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 79/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - accuracy: 0.8047 - loss: 0.3540 - precision: 0.8410 - recall: 0.7483 - val_accuracy: 0.8750 - val_loss: 0.2291 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 80/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8750 - loss: 0.1915 - precision: 1.0000 - recall: 0.7500 - val_accuracy: 1.0000 - val_loss: 0.1839 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 81/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.8410 - loss: 0.3218 - precision: 0.8912 - recall: 0.7977 - val_accuracy: 0.8125 - val_loss: 0.2095 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 82/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8750 - loss: 0.2784 - precision: 0.6667 - recall: 1.0000 - val_accuracy: 0.8750 - val_loss: 0.1710 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "Epoch 83/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - accuracy: 0.9155 - loss: 0.2652 - precision: 0.8791 - recall: 0.9568 - val_accuracy: 0.9375 - val_loss: 0.2092 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 84/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1295 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.8125 - val_loss: 0.3059 - val_precision: 1.0000 - val_recall: 0.4000\n",
            "Epoch 85/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - accuracy: 0.8759 - loss: 0.2648 - precision: 0.8802 - recall: 0.8813 - val_accuracy: 0.9375 - val_loss: 0.1458 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 86/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7500 - loss: 0.4028 - precision: 1.0000 - recall: 0.7143 - val_accuracy: 0.9375 - val_loss: 0.2390 - val_precision: 1.0000 - val_recall: 0.8000\n",
            "Epoch 87/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 129ms/step - accuracy: 0.9265 - loss: 0.1949 - precision: 0.9624 - recall: 0.8976 - val_accuracy: 1.0000 - val_loss: 0.1293 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 88/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1809 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 0.1819 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 89/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - accuracy: 0.9308 - loss: 0.2178 - precision: 0.9442 - recall: 0.9068 - val_accuracy: 0.8750 - val_loss: 0.2003 - val_precision: 1.0000 - val_recall: 0.6000\n",
            "\u001b[1m2/3\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 77ms/stepWARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002A9A2BB1C60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002A9A2BB1C60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 719ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.73      1.00      0.85        11\n",
            "        REAL       1.00      0.64      0.78        11\n",
            "\n",
            "    accuracy                           0.82        22\n",
            "   macro avg       0.87      0.82      0.81        22\n",
            "weighted avg       0.87      0.82      0.81        22\n",
            "\n",
            "\n",
            "Evaluation visualizations saved:\n",
            "- confusion_matrix.png\n",
            "- roc_curve.png\n",
            "- training_history.png\n"
          ]
        }
      ]
    }
  ]
}