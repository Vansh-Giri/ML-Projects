{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(r'D:\\ML\\minor project')\n",
        "print(\"Current working directory:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOs3ZAEM-VJW",
        "outputId": "0a7de4d4-7704-402d-ddef-8974bbf8bc0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: D:\\ML\\minor project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchaudio==2.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3bWDORw-6zd",
        "outputId": "b7ba2347-06d7-4e00-e3fa-efc9da28dae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.6.0\n",
            "  Downloading torchaudio-2.6.0-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: torch==2.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchaudio==2.6.0) (2.6.0)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.6.0->torchaudio==2.6.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.6.0->torchaudio==2.6.0) (4.11.0)\n",
            "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.6.0->torchaudio==2.6.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.6.0->torchaudio==2.6.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.6.0->torchaudio==2.6.0) (2024.6.1)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.6.0->torchaudio==2.6.0) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.6.0->torchaudio==2.6.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch==2.6.0->torchaudio==2.6.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch==2.6.0->torchaudio==2.6.0) (2.1.3)\n",
            "Downloading torchaudio-2.6.0-cp312-cp312-win_amd64.whl (2.4 MB)\n",
            "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.5/2.4 MB 2.1 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 0.5/2.4 MB 2.1 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 1.0/2.4 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 1.6/2.4 MB 1.7 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 2.4/2.4 MB 2.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.4/2.4 MB 2.2 MB/s eta 0:00:00\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "\n",
        "def analyze_audio_files(audio_path):\n",
        "    min_length = float('inf')\n",
        "    sample_rates = set()\n",
        "\n",
        "    for label in ['FAKE', 'REAL']:\n",
        "        label_path = os.path.join(audio_path, label)\n",
        "        if not os.path.exists(label_path):\n",
        "            print(f\"Directory not found: {label_path}\")\n",
        "            continue\n",
        "\n",
        "        for file in os.listdir(label_path):\n",
        "            if file.endswith('.wav'):\n",
        "                file_path = os.path.join(label_path, file)\n",
        "                try:\n",
        "                    # Load audio with original sample rate\n",
        "                    y, sr = librosa.load(file_path, sr=None)\n",
        "                    duration = len(y)  # Total samples\n",
        "                    sample_rates.add(sr)\n",
        "\n",
        "                    if duration < min_length:\n",
        "                        min_length = duration\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "    return min_length, sample_rates\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    AUDIO_PATH = r\"D:\\ML\\minor project\\archive\\SPLITTED\"\n",
        "\n",
        "    if not os.path.exists(AUDIO_PATH):\n",
        "        print(f\"Audio directory not found: {AUDIO_PATH}\")\n",
        "        exit(1)\n",
        "\n",
        "    min_samples, sample_rates = analyze_audio_files(AUDIO_PATH)\n",
        "\n",
        "    print(\"\\nAnalysis Results:\")\n",
        "    print(f\"Minimum audio length (samples): {min_samples}\")\n",
        "    print(f\"Sample rates found: {sample_rates}\")\n",
        "\n",
        "    # Recommended n_fft settings\n",
        "    common_sr = sample_rates.pop() if sample_rates else 16000\n",
        "    recommended_n_fft = {\n",
        "        'music': 2048,       # Typical for music @ 22050Hz\n",
        "        'speech': 512,       # Typical for speech processing\n",
        "        'custom': min(min_samples, 1024)  # Safe value based on your shortest file\n",
        "    }\n",
        "\n",
        "    print(\"\\nRecommended n_fft values:\")\n",
        "    print(f\"- For music: {recommended_n_fft['music']}\")\n",
        "    print(f\"- For speech: {recommended_n_fft['speech']}\")\n",
        "    print(f\"- Safe custom: {recommended_n_fft['custom']} (based on your shortest file)\")\n",
        "\n",
        "    if min_samples < 512:\n",
        "        print(\"\\nWarning: Very short audio files detected!\")\n",
        "        print(\"Consider:\")\n",
        "        print(\"- Padding shorter files with silence\")\n",
        "        print(\"- Using smaller n_fft (e.g., 256)\")\n",
        "        print(\"- Checking file lengths with librosa.get_duration()\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L6BtFVkB5oh",
        "outputId": "24099755-ffbd-40fb-bcf9-e403fe05939a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analysis Results:\n",
            "Minimum audio length (samples): 144\n",
            "Sample rates found: {40000, 44100, 48000}\n",
            "\n",
            "Recommended n_fft values:\n",
            "- For music: 2048\n",
            "- For speech: 512\n",
            "- Safe custom: 144 (based on your shortest file)\n",
            "\n",
            "Warning: Very short audio files detected!\n",
            "Consider:\n",
            "- Padding shorter files with silence\n",
            "- Using smaller n_fft (e.g., 256)\n",
            "- Checking file lengths with librosa.get_duration()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "wpMlQbPb-DTO",
        "outputId": "ef2bfb25-b6e1-4c2d-8e9e-430b0cf6d617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (129) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [1, 129, 128, 6887] at entry 0 and [1, 128, 6887] at entry 1",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 229\u001b[0m\n\u001b[0;32m    226\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconformer_model_final.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 229\u001b[0m     train_model()\n",
            "Cell \u001b[1;32mIn[17], line 200\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m    199\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m specs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m    201\u001b[0m         specs \u001b[38;5;241m=\u001b[39m specs\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m    202\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
            "Cell \u001b[1;32mIn[17], line 46\u001b[0m, in \u001b[0;36mpad_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     43\u001b[0m         spec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(spec, (\u001b[38;5;241m0\u001b[39m, max_time \u001b[38;5;241m-\u001b[39m spec\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     44\u001b[0m     padded_specs\u001b[38;5;241m.\u001b[39mappend(spec)\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(padded_specs), torch\u001b[38;5;241m.\u001b[39mtensor(labels)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 129, 128, 6887] at entry 0 and [1, 128, 6887] at entry 1"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Audio Deepfake Detection with Conformer\n",
        "- Fixed tensor dimensions\n",
        "- Enhanced augmentation handling\n",
        "- Custom collate function\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchaudio.transforms import MelSpectrogram, TimeStretch, Resample\n",
        "from torchaudio.models.conformer import Conformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Configuration\n",
        "AUDIO_PATH = r\"D:\\ML\\minor project\\archive\\SPLITTED\"\n",
        "TARGET_SAMPLE_RATE = 44100\n",
        "REQUIRED_LENGTH = 10 * TARGET_SAMPLE_RATE  # 441,000 samples\n",
        "N_FFT = 256\n",
        "WIN_LENGTH = 256\n",
        "HOP_LENGTH = 64\n",
        "N_MELS = 128\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 50\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "FIXED_TIME_STEPS = (REQUIRED_LENGTH - N_FFT) // HOP_LENGTH + 1  # 6887\n",
        "\n",
        "def pad_collate(batch):\n",
        "    \"\"\"Custom collate function to handle final dimension mismatches\"\"\"\n",
        "    specs, labels = zip(*batch)\n",
        "\n",
        "    # Get max time dimension\n",
        "    max_time = max(spec.shape[-1] for spec in specs)\n",
        "\n",
        "    # Pad all specs to max_time\n",
        "    padded_specs = []\n",
        "    for spec in specs:\n",
        "        if spec.shape[-1] < max_time:\n",
        "            spec = torch.nn.functional.pad(spec, (0, max_time - spec.shape[-1]))\n",
        "        padded_specs.append(spec)\n",
        "\n",
        "    return torch.stack(padded_specs), torch.tensor(labels)\n",
        "\n",
        "def is_valid_file(file_path):\n",
        "    \"\"\"Check if audio file meets requirements\"\"\"\n",
        "    try:\n",
        "        info = torchaudio.info(file_path)\n",
        "        return info.num_frames == REQUIRED_LENGTH\n",
        "    except Exception as e:\n",
        "        print(f\"Invalid file {file_path}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Load and filter data with proper augmentation\"\"\"\n",
        "    fake_files = [os.path.join(AUDIO_PATH, \"FAKE\", f)\n",
        "                 for f in os.listdir(os.path.join(AUDIO_PATH, \"FAKE\"))\n",
        "                 if is_valid_file(os.path.join(AUDIO_PATH, \"FAKE\", f))]\n",
        "\n",
        "    real_files = [os.path.join(AUDIO_PATH, \"REAL\", f)\n",
        "                 for f in os.listdir(os.path.join(AUDIO_PATH, \"REAL\"))\n",
        "                 if is_valid_file(os.path.join(AUDIO_PATH, \"REAL\", f))]\n",
        "\n",
        "    # Apply 7x augmentation to REAL class after validation\n",
        "    real_files_augmented = real_files * 7\n",
        "    labels = [0]*len(fake_files) + [1]*len(real_files_augmented)\n",
        "\n",
        "    return train_test_split(\n",
        "        fake_files + real_files_augmented,\n",
        "        labels,\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, files, labels, augment=False):\n",
        "        self.files = files\n",
        "        self.labels = labels\n",
        "        self.augment = augment\n",
        "\n",
        "        # Audio transforms\n",
        "        self.resample = Resample(orig_freq=TARGET_SAMPLE_RATE, new_freq=TARGET_SAMPLE_RATE)\n",
        "        self.mel_spec = MelSpectrogram(\n",
        "            sample_rate=TARGET_SAMPLE_RATE,\n",
        "            n_fft=N_FFT,\n",
        "            win_length=WIN_LENGTH,\n",
        "            hop_length=HOP_LENGTH,\n",
        "            n_mels=N_MELS,\n",
        "            normalized=True\n",
        "        )\n",
        "        self.time_stretch = TimeStretch(\n",
        "            hop_length=HOP_LENGTH,\n",
        "            n_freq=N_FFT//2 + 1\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file = self.files[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load and process\n",
        "        waveform, _ = torchaudio.load(file)\n",
        "        waveform = self.resample(waveform)\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)\n",
        "        waveform = waveform / (waveform.abs().max() + 1e-9)\n",
        "\n",
        "        # Base Mel spectrogram\n",
        "        mel = self.mel_spec(waveform)\n",
        "\n",
        "        # Augmentation (REAL only)\n",
        "        if self.augment and label == 1:\n",
        "            spec = torch.stft(\n",
        "                waveform,\n",
        "                n_fft=N_FFT,\n",
        "                hop_length=HOP_LENGTH,\n",
        "                win_length=WIN_LENGTH,\n",
        "                window=torch.hann_window(WIN_LENGTH),\n",
        "                return_complex=True\n",
        "            )\n",
        "\n",
        "            # Time stretch with strict length enforcement\n",
        "            rate = np.random.uniform(0.9, 1.1)\n",
        "            spec = self.time_stretch(spec, rate)\n",
        "            spec = spec[..., :FIXED_TIME_STEPS]  # Hard trim to exact size\n",
        "\n",
        "            # Regenerate Mel from stretched spec\n",
        "            mel = self.mel_spec(torch.abs(spec))\n",
        "\n",
        "        # Final shape enforcement\n",
        "        if mel.shape[-1] != FIXED_TIME_STEPS:\n",
        "            mel = mel[..., :FIXED_TIME_STEPS]  # Trim to exact size\n",
        "\n",
        "        return torchaudio.transforms.AmplitudeToDB()(mel), label\n",
        "\n",
        "class AudioConformer(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.conformer = Conformer(\n",
        "            input_dim=N_MELS,\n",
        "            num_heads=4,\n",
        "            ffn_dim=256,\n",
        "            num_layers=4,\n",
        "            depthwise_conv_kernel_size=63\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        x = x.squeeze(1)  # [B,1,F,T] -> [B,F,T]\n",
        "        x = x.permute(0,2,1)  # [B,T,F] for Conformer\n",
        "        x, _ = self.conformer(x, lengths)\n",
        "        x = x.permute(0,2,1)  # [B,F,T] for pooling\n",
        "        return self.classifier(x.unsqueeze(1))\n",
        "\n",
        "def create_weighted_sampler(labels):\n",
        "    class_counts = np.bincount(labels)\n",
        "    class_weights = 1. / torch.Tensor(class_counts)\n",
        "    sample_weights = class_weights[labels]\n",
        "    return WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "\n",
        "def train_model():\n",
        "    train_files, test_files, train_labels, test_labels = load_data()\n",
        "\n",
        "    train_dataset = AudioDataset(train_files, train_labels, augment=True)\n",
        "    test_dataset = AudioDataset(test_files, test_labels)\n",
        "\n",
        "    sampler = create_weighted_sampler(train_dataset.labels)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=sampler,\n",
        "        collate_fn=pad_collate  # Custom collate\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        collate_fn=pad_collate  # Custom collate\n",
        "    )\n",
        "\n",
        "    model = AudioConformer().to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 3.0]).to(DEVICE))\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "    # Fixed lengths for all samples\n",
        "    lengths = torch.full((BATCH_SIZE,), FIXED_TIME_STEPS,\n",
        "                       dtype=torch.long, device=DEVICE)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        for specs, labels in train_loader:\n",
        "            specs = specs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            outputs = model(specs, lengths[:specs.size(0)])\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for specs, labels in test_loader:\n",
        "                specs = specs.to(DEVICE)\n",
        "                outputs = model(specs, lengths[:specs.size(0)])\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.numpy())\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "        print(classification_report(all_labels, all_preds, target_names=[\"FAKE\", \"REAL\"]))\n",
        "        print(f\"ROC AUC: {roc_auc_score(all_labels, all_preds):.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"conformer_model_final.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()\n"
      ]
    }
  ]
}