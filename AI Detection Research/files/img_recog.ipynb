{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 1: Install KaggleHub and Download Dataset\n",
        "!pip install kagglehub\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download dataset directly from Kaggle\n",
        "path = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__I1QTIJ1ZDt",
        "outputId": "369db1c3-b351-4d49-cdee-1188a1925b39"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/birdy654/cifake-real-and-ai-generated-synthetic-images?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 105M/105M [00:00<00:00, 135MB/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images/versions/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ✅ Step 2: Import Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ✅ Step 3: Device Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# ✅ Step 4: Data Paths (Now using the dataset downloaded from Kaggle)\n",
        "train_dir = path + '/train'  # Dataset train folder\n",
        "test_dir = path + '/test'    # Dataset test folder\n",
        "\n",
        "# ✅ Step 5: Data Augmentation and Normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),       # Upscale 32x32 to 128x128\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])   # ImageNet normalization\n",
        "])\n",
        "\n",
        "# ✅ Step 6: Load Dataset\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# ✅ Step 7: Model Setup\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)  # 2 classes: fake, real\n",
        "model = model.to(device)\n",
        "\n",
        "# ✅ Step 8: Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ✅ Step 9: Training Loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 100 == 0:  # Log every 100 iterations\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "# ✅ Step 10: Evaluation\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "y_prob = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        probs = torch.softmax(outputs, dim=1)[:,1]\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_prob.extend(probs.cpu().numpy())\n",
        "\n",
        "# ✅ Step 11: Metrics\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "prec = precision_score(y_true, y_pred)\n",
        "rec = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "roc = roc_auc_score(y_true, y_prob)\n",
        "\n",
        "print(f'Accuracy: {acc:.4f}')\n",
        "print(f'Precision: {prec:.4f}')\n",
        "print(f'Recall: {rec:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "print(f'ROC-AUC: {roc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLZQxTfj1W7n",
        "outputId": "078db92f-d3d8-4549-892d-f18b2a69d54f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [0/3125], Loss: 0.0067\n",
            "Epoch [1/10], Step [100/3125], Loss: 0.2656\n",
            "Epoch [1/10], Step [200/3125], Loss: 0.1672\n",
            "Epoch [1/10], Step [300/3125], Loss: 0.1512\n",
            "Epoch [1/10], Step [400/3125], Loss: 0.1404\n",
            "Epoch [1/10], Step [500/3125], Loss: 0.1396\n",
            "Epoch [1/10], Step [600/3125], Loss: 0.1209\n",
            "Epoch [1/10], Step [700/3125], Loss: 0.1176\n",
            "Epoch [1/10], Step [800/3125], Loss: 0.1139\n",
            "Epoch [1/10], Step [900/3125], Loss: 0.1147\n",
            "Epoch [1/10], Step [1000/3125], Loss: 0.1204\n",
            "Epoch [1/10], Step [1100/3125], Loss: 0.1068\n",
            "Epoch [1/10], Step [1200/3125], Loss: 0.1065\n",
            "Epoch [1/10], Step [1300/3125], Loss: 0.1022\n",
            "Epoch [1/10], Step [1400/3125], Loss: 0.0882\n",
            "Epoch [1/10], Step [1500/3125], Loss: 0.1102\n",
            "Epoch [1/10], Step [1600/3125], Loss: 0.1050\n",
            "Epoch [1/10], Step [1700/3125], Loss: 0.0920\n",
            "Epoch [1/10], Step [1800/3125], Loss: 0.0988\n",
            "Epoch [1/10], Step [1900/3125], Loss: 0.0932\n",
            "Epoch [1/10], Step [2000/3125], Loss: 0.0912\n",
            "Epoch [1/10], Step [2100/3125], Loss: 0.0981\n",
            "Epoch [1/10], Step [2200/3125], Loss: 0.0798\n",
            "Epoch [1/10], Step [2300/3125], Loss: 0.1099\n",
            "Epoch [1/10], Step [2400/3125], Loss: 0.0851\n",
            "Epoch [1/10], Step [2500/3125], Loss: 0.1045\n",
            "Epoch [1/10], Step [2600/3125], Loss: 0.0922\n",
            "Epoch [1/10], Step [2700/3125], Loss: 0.1025\n",
            "Epoch [1/10], Step [2800/3125], Loss: 0.1042\n",
            "Epoch [1/10], Step [2900/3125], Loss: 0.0756\n",
            "Epoch [1/10], Step [3000/3125], Loss: 0.0841\n",
            "Epoch [1/10], Step [3100/3125], Loss: 0.0888\n",
            "Epoch [1/10], Loss: 0.0008\n",
            "Epoch [2/10], Step [0/3125], Loss: 0.0005\n",
            "Epoch [2/10], Step [100/3125], Loss: 0.0777\n",
            "Epoch [2/10], Step [200/3125], Loss: 0.0796\n",
            "Epoch [2/10], Step [300/3125], Loss: 0.0758\n",
            "Epoch [2/10], Step [400/3125], Loss: 0.0617\n",
            "Epoch [2/10], Step [500/3125], Loss: 0.0693\n",
            "Epoch [2/10], Step [600/3125], Loss: 0.0762\n",
            "Epoch [2/10], Step [700/3125], Loss: 0.0709\n",
            "Epoch [2/10], Step [800/3125], Loss: 0.0807\n",
            "Epoch [2/10], Step [900/3125], Loss: 0.0675\n",
            "Epoch [2/10], Step [1000/3125], Loss: 0.0792\n",
            "Epoch [2/10], Step [1100/3125], Loss: 0.0642\n",
            "Epoch [2/10], Step [1200/3125], Loss: 0.0630\n",
            "Epoch [2/10], Step [1300/3125], Loss: 0.0781\n",
            "Epoch [2/10], Step [1400/3125], Loss: 0.0650\n",
            "Epoch [2/10], Step [1500/3125], Loss: 0.0677\n",
            "Epoch [2/10], Step [1600/3125], Loss: 0.0786\n",
            "Epoch [2/10], Step [1700/3125], Loss: 0.0727\n",
            "Epoch [2/10], Step [1800/3125], Loss: 0.0619\n",
            "Epoch [2/10], Step [1900/3125], Loss: 0.0673\n",
            "Epoch [2/10], Step [2000/3125], Loss: 0.0659\n",
            "Epoch [2/10], Step [2100/3125], Loss: 0.0670\n",
            "Epoch [2/10], Step [2200/3125], Loss: 0.0643\n",
            "Epoch [2/10], Step [2300/3125], Loss: 0.0705\n",
            "Epoch [2/10], Step [2400/3125], Loss: 0.0681\n",
            "Epoch [2/10], Step [2500/3125], Loss: 0.0759\n",
            "Epoch [2/10], Step [2600/3125], Loss: 0.0623\n",
            "Epoch [2/10], Step [2700/3125], Loss: 0.0814\n",
            "Epoch [2/10], Step [2800/3125], Loss: 0.0592\n",
            "Epoch [2/10], Step [2900/3125], Loss: 0.0704\n",
            "Epoch [2/10], Step [3000/3125], Loss: 0.0808\n",
            "Epoch [2/10], Step [3100/3125], Loss: 0.0655\n",
            "Epoch [2/10], Loss: 0.0006\n",
            "Epoch [3/10], Step [0/3125], Loss: 0.0002\n",
            "Epoch [3/10], Step [100/3125], Loss: 0.0464\n",
            "Epoch [3/10], Step [200/3125], Loss: 0.0464\n",
            "Epoch [3/10], Step [300/3125], Loss: 0.0665\n",
            "Epoch [3/10], Step [400/3125], Loss: 0.0470\n",
            "Epoch [3/10], Step [500/3125], Loss: 0.0453\n",
            "Epoch [3/10], Step [600/3125], Loss: 0.0640\n",
            "Epoch [3/10], Step [700/3125], Loss: 0.0673\n",
            "Epoch [3/10], Step [800/3125], Loss: 0.0450\n",
            "Epoch [3/10], Step [900/3125], Loss: 0.0471\n",
            "Epoch [3/10], Step [1000/3125], Loss: 0.0557\n",
            "Epoch [3/10], Step [1100/3125], Loss: 0.0508\n",
            "Epoch [3/10], Step [1200/3125], Loss: 0.0527\n",
            "Epoch [3/10], Step [1300/3125], Loss: 0.0691\n",
            "Epoch [3/10], Step [1400/3125], Loss: 0.0568\n",
            "Epoch [3/10], Step [1500/3125], Loss: 0.0596\n",
            "Epoch [3/10], Step [1600/3125], Loss: 0.0566\n",
            "Epoch [3/10], Step [1700/3125], Loss: 0.0579\n",
            "Epoch [3/10], Step [1800/3125], Loss: 0.0583\n",
            "Epoch [3/10], Step [1900/3125], Loss: 0.0625\n",
            "Epoch [3/10], Step [2000/3125], Loss: 0.0546\n",
            "Epoch [3/10], Step [2100/3125], Loss: 0.0691\n",
            "Epoch [3/10], Step [2200/3125], Loss: 0.0647\n",
            "Epoch [3/10], Step [2300/3125], Loss: 0.0516\n",
            "Epoch [3/10], Step [2400/3125], Loss: 0.0506\n",
            "Epoch [3/10], Step [2500/3125], Loss: 0.0592\n",
            "Epoch [3/10], Step [2600/3125], Loss: 0.0609\n",
            "Epoch [3/10], Step [2700/3125], Loss: 0.0552\n",
            "Epoch [3/10], Step [2800/3125], Loss: 0.0528\n",
            "Epoch [3/10], Step [2900/3125], Loss: 0.0546\n",
            "Epoch [3/10], Step [3000/3125], Loss: 0.0607\n",
            "Epoch [3/10], Step [3100/3125], Loss: 0.0582\n",
            "Epoch [3/10], Loss: 0.0005\n",
            "Epoch [4/10], Step [0/3125], Loss: 0.0004\n",
            "Epoch [4/10], Step [100/3125], Loss: 0.0375\n",
            "Epoch [4/10], Step [200/3125], Loss: 0.0387\n",
            "Epoch [4/10], Step [300/3125], Loss: 0.0646\n",
            "Epoch [4/10], Step [400/3125], Loss: 0.0435\n",
            "Epoch [4/10], Step [500/3125], Loss: 0.0382\n",
            "Epoch [4/10], Step [600/3125], Loss: 0.0373\n",
            "Epoch [4/10], Step [700/3125], Loss: 0.0515\n",
            "Epoch [4/10], Step [800/3125], Loss: 0.0451\n",
            "Epoch [4/10], Step [900/3125], Loss: 0.0457\n",
            "Epoch [4/10], Step [1000/3125], Loss: 0.0461\n",
            "Epoch [4/10], Step [1100/3125], Loss: 0.0479\n",
            "Epoch [4/10], Step [1200/3125], Loss: 0.0648\n",
            "Epoch [4/10], Step [1300/3125], Loss: 0.0395\n",
            "Epoch [4/10], Step [1400/3125], Loss: 0.0349\n",
            "Epoch [4/10], Step [1500/3125], Loss: 0.0552\n",
            "Epoch [4/10], Step [1600/3125], Loss: 0.0438\n",
            "Epoch [4/10], Step [1700/3125], Loss: 0.0462\n",
            "Epoch [4/10], Step [1800/3125], Loss: 0.0405\n",
            "Epoch [4/10], Step [1900/3125], Loss: 0.0459\n",
            "Epoch [4/10], Step [2000/3125], Loss: 0.0549\n",
            "Epoch [4/10], Step [2100/3125], Loss: 0.0471\n",
            "Epoch [4/10], Step [2200/3125], Loss: 0.0439\n",
            "Epoch [4/10], Step [2300/3125], Loss: 0.0527\n",
            "Epoch [4/10], Step [2400/3125], Loss: 0.0273\n",
            "Epoch [4/10], Step [2500/3125], Loss: 0.0480\n",
            "Epoch [4/10], Step [2600/3125], Loss: 0.0465\n",
            "Epoch [4/10], Step [2700/3125], Loss: 0.0485\n",
            "Epoch [4/10], Step [2800/3125], Loss: 0.0506\n",
            "Epoch [4/10], Step [2900/3125], Loss: 0.0487\n",
            "Epoch [4/10], Step [3000/3125], Loss: 0.0413\n",
            "Epoch [4/10], Step [3100/3125], Loss: 0.0507\n",
            "Epoch [4/10], Loss: 0.0006\n",
            "Epoch [5/10], Step [0/3125], Loss: 0.0005\n",
            "Epoch [5/10], Step [100/3125], Loss: 0.0350\n",
            "Epoch [5/10], Step [200/3125], Loss: 0.0414\n",
            "Epoch [5/10], Step [300/3125], Loss: 0.0370\n",
            "Epoch [5/10], Step [400/3125], Loss: 0.0444\n",
            "Epoch [5/10], Step [500/3125], Loss: 0.0350\n",
            "Epoch [5/10], Step [600/3125], Loss: 0.0377\n",
            "Epoch [5/10], Step [700/3125], Loss: 0.0304\n",
            "Epoch [5/10], Step [800/3125], Loss: 0.0324\n",
            "Epoch [5/10], Step [900/3125], Loss: 0.0393\n",
            "Epoch [5/10], Step [1000/3125], Loss: 0.0293\n",
            "Epoch [5/10], Step [1100/3125], Loss: 0.0358\n",
            "Epoch [5/10], Step [1200/3125], Loss: 0.0374\n",
            "Epoch [5/10], Step [1300/3125], Loss: 0.0445\n",
            "Epoch [5/10], Step [1400/3125], Loss: 0.0551\n",
            "Epoch [5/10], Step [1500/3125], Loss: 0.0377\n",
            "Epoch [5/10], Step [1600/3125], Loss: 0.0366\n",
            "Epoch [5/10], Step [1700/3125], Loss: 0.0304\n",
            "Epoch [5/10], Step [1800/3125], Loss: 0.0367\n",
            "Epoch [5/10], Step [1900/3125], Loss: 0.0534\n",
            "Epoch [5/10], Step [2000/3125], Loss: 0.0409\n",
            "Epoch [5/10], Step [2100/3125], Loss: 0.0303\n",
            "Epoch [5/10], Step [2200/3125], Loss: 0.0474\n",
            "Epoch [5/10], Step [2300/3125], Loss: 0.0346\n",
            "Epoch [5/10], Step [2400/3125], Loss: 0.0398\n",
            "Epoch [5/10], Step [2500/3125], Loss: 0.0296\n",
            "Epoch [5/10], Step [2600/3125], Loss: 0.0402\n",
            "Epoch [5/10], Step [2700/3125], Loss: 0.0299\n",
            "Epoch [5/10], Step [2800/3125], Loss: 0.0432\n",
            "Epoch [5/10], Step [2900/3125], Loss: 0.0381\n",
            "Epoch [5/10], Step [3000/3125], Loss: 0.0398\n",
            "Epoch [5/10], Step [3100/3125], Loss: 0.0286\n",
            "Epoch [5/10], Loss: 0.0001\n",
            "Epoch [6/10], Step [0/3125], Loss: 0.0014\n",
            "Epoch [6/10], Step [100/3125], Loss: 0.0304\n",
            "Epoch [6/10], Step [200/3125], Loss: 0.0322\n",
            "Epoch [6/10], Step [300/3125], Loss: 0.0224\n",
            "Epoch [6/10], Step [400/3125], Loss: 0.0351\n",
            "Epoch [6/10], Step [500/3125], Loss: 0.0280\n",
            "Epoch [6/10], Step [600/3125], Loss: 0.0393\n",
            "Epoch [6/10], Step [700/3125], Loss: 0.0262\n",
            "Epoch [6/10], Step [800/3125], Loss: 0.0420\n",
            "Epoch [6/10], Step [900/3125], Loss: 0.0306\n",
            "Epoch [6/10], Step [1000/3125], Loss: 0.0324\n",
            "Epoch [6/10], Step [1100/3125], Loss: 0.0357\n",
            "Epoch [6/10], Step [1200/3125], Loss: 0.0305\n",
            "Epoch [6/10], Step [1300/3125], Loss: 0.0347\n",
            "Epoch [6/10], Step [1400/3125], Loss: 0.0390\n",
            "Epoch [6/10], Step [1500/3125], Loss: 0.0297\n",
            "Epoch [6/10], Step [1600/3125], Loss: 0.0355\n",
            "Epoch [6/10], Step [1700/3125], Loss: 0.0374\n",
            "Epoch [6/10], Step [1800/3125], Loss: 0.0307\n",
            "Epoch [6/10], Step [1900/3125], Loss: 0.0390\n",
            "Epoch [6/10], Step [2000/3125], Loss: 0.0287\n",
            "Epoch [6/10], Step [2100/3125], Loss: 0.0334\n",
            "Epoch [6/10], Step [2200/3125], Loss: 0.0338\n",
            "Epoch [6/10], Step [2300/3125], Loss: 0.0298\n",
            "Epoch [6/10], Step [2400/3125], Loss: 0.0266\n",
            "Epoch [6/10], Step [2500/3125], Loss: 0.0320\n",
            "Epoch [6/10], Step [2600/3125], Loss: 0.0489\n",
            "Epoch [6/10], Step [2700/3125], Loss: 0.0252\n",
            "Epoch [6/10], Step [2800/3125], Loss: 0.0285\n",
            "Epoch [6/10], Step [2900/3125], Loss: 0.0332\n",
            "Epoch [6/10], Step [3000/3125], Loss: 0.0434\n",
            "Epoch [6/10], Step [3100/3125], Loss: 0.0340\n",
            "Epoch [6/10], Loss: 0.0003\n",
            "Epoch [7/10], Step [0/3125], Loss: 0.0002\n",
            "Epoch [7/10], Step [100/3125], Loss: 0.0262\n",
            "Epoch [7/10], Step [200/3125], Loss: 0.0238\n",
            "Epoch [7/10], Step [300/3125], Loss: 0.0145\n",
            "Epoch [7/10], Step [400/3125], Loss: 0.0226\n",
            "Epoch [7/10], Step [500/3125], Loss: 0.0222\n",
            "Epoch [7/10], Step [600/3125], Loss: 0.0311\n",
            "Epoch [7/10], Step [700/3125], Loss: 0.0294\n",
            "Epoch [7/10], Step [800/3125], Loss: 0.0355\n",
            "Epoch [7/10], Step [900/3125], Loss: 0.0262\n",
            "Epoch [7/10], Step [1000/3125], Loss: 0.0273\n",
            "Epoch [7/10], Step [1100/3125], Loss: 0.0290\n",
            "Epoch [7/10], Step [1200/3125], Loss: 0.0278\n",
            "Epoch [7/10], Step [1300/3125], Loss: 0.0385\n",
            "Epoch [7/10], Step [1400/3125], Loss: 0.0244\n",
            "Epoch [7/10], Step [1500/3125], Loss: 0.0296\n",
            "Epoch [7/10], Step [1600/3125], Loss: 0.0199\n",
            "Epoch [7/10], Step [1700/3125], Loss: 0.0247\n",
            "Epoch [7/10], Step [1800/3125], Loss: 0.0263\n",
            "Epoch [7/10], Step [1900/3125], Loss: 0.0320\n",
            "Epoch [7/10], Step [2000/3125], Loss: 0.0246\n",
            "Epoch [7/10], Step [2100/3125], Loss: 0.0266\n",
            "Epoch [7/10], Step [2200/3125], Loss: 0.0354\n",
            "Epoch [7/10], Step [2300/3125], Loss: 0.0293\n",
            "Epoch [7/10], Step [2400/3125], Loss: 0.0284\n",
            "Epoch [7/10], Step [2500/3125], Loss: 0.0253\n",
            "Epoch [7/10], Step [2600/3125], Loss: 0.0290\n",
            "Epoch [7/10], Step [2700/3125], Loss: 0.0317\n",
            "Epoch [7/10], Step [2800/3125], Loss: 0.0216\n",
            "Epoch [7/10], Step [2900/3125], Loss: 0.0305\n",
            "Epoch [7/10], Step [3000/3125], Loss: 0.0308\n",
            "Epoch [7/10], Step [3100/3125], Loss: 0.0295\n",
            "Epoch [7/10], Loss: 0.0003\n",
            "Epoch [8/10], Step [0/3125], Loss: 0.0008\n",
            "Epoch [8/10], Step [100/3125], Loss: 0.0237\n",
            "Epoch [8/10], Step [200/3125], Loss: 0.0220\n",
            "Epoch [8/10], Step [300/3125], Loss: 0.0295\n",
            "Epoch [8/10], Step [400/3125], Loss: 0.0221\n",
            "Epoch [8/10], Step [500/3125], Loss: 0.0176\n",
            "Epoch [8/10], Step [600/3125], Loss: 0.0267\n",
            "Epoch [8/10], Step [700/3125], Loss: 0.0200\n",
            "Epoch [8/10], Step [800/3125], Loss: 0.0280\n",
            "Epoch [8/10], Step [900/3125], Loss: 0.0247\n",
            "Epoch [8/10], Step [1000/3125], Loss: 0.0251\n",
            "Epoch [8/10], Step [1100/3125], Loss: 0.0281\n",
            "Epoch [8/10], Step [1200/3125], Loss: 0.0232\n",
            "Epoch [8/10], Step [1300/3125], Loss: 0.0278\n",
            "Epoch [8/10], Step [1400/3125], Loss: 0.0328\n",
            "Epoch [8/10], Step [1500/3125], Loss: 0.0154\n",
            "Epoch [8/10], Step [1600/3125], Loss: 0.0200\n",
            "Epoch [8/10], Step [1700/3125], Loss: 0.0182\n",
            "Epoch [8/10], Step [1800/3125], Loss: 0.0310\n",
            "Epoch [8/10], Step [1900/3125], Loss: 0.0281\n",
            "Epoch [8/10], Step [2000/3125], Loss: 0.0265\n",
            "Epoch [8/10], Step [2100/3125], Loss: 0.0328\n",
            "Epoch [8/10], Step [2200/3125], Loss: 0.0215\n",
            "Epoch [8/10], Step [2300/3125], Loss: 0.0350\n",
            "Epoch [8/10], Step [2400/3125], Loss: 0.0232\n",
            "Epoch [8/10], Step [2500/3125], Loss: 0.0282\n",
            "Epoch [8/10], Step [2600/3125], Loss: 0.0221\n",
            "Epoch [8/10], Step [2700/3125], Loss: 0.0192\n",
            "Epoch [8/10], Step [2800/3125], Loss: 0.0312\n",
            "Epoch [8/10], Step [2900/3125], Loss: 0.0251\n",
            "Epoch [8/10], Step [3000/3125], Loss: 0.0215\n",
            "Epoch [8/10], Step [3100/3125], Loss: 0.0206\n",
            "Epoch [8/10], Loss: 0.0003\n",
            "Epoch [9/10], Step [0/3125], Loss: 0.0002\n",
            "Epoch [9/10], Step [100/3125], Loss: 0.0191\n",
            "Epoch [9/10], Step [200/3125], Loss: 0.0197\n",
            "Epoch [9/10], Step [300/3125], Loss: 0.0236\n",
            "Epoch [9/10], Step [400/3125], Loss: 0.0188\n",
            "Epoch [9/10], Step [500/3125], Loss: 0.0209\n",
            "Epoch [9/10], Step [600/3125], Loss: 0.0261\n",
            "Epoch [9/10], Step [700/3125], Loss: 0.0138\n",
            "Epoch [9/10], Step [800/3125], Loss: 0.0209\n",
            "Epoch [9/10], Step [900/3125], Loss: 0.0144\n",
            "Epoch [9/10], Step [1000/3125], Loss: 0.0351\n",
            "Epoch [9/10], Step [1100/3125], Loss: 0.0215\n",
            "Epoch [9/10], Step [1200/3125], Loss: 0.0173\n",
            "Epoch [9/10], Step [1300/3125], Loss: 0.0244\n",
            "Epoch [9/10], Step [1400/3125], Loss: 0.0192\n",
            "Epoch [9/10], Step [1500/3125], Loss: 0.0284\n",
            "Epoch [9/10], Step [1600/3125], Loss: 0.0219\n",
            "Epoch [9/10], Step [1700/3125], Loss: 0.0187\n",
            "Epoch [9/10], Step [1800/3125], Loss: 0.0201\n",
            "Epoch [9/10], Step [1900/3125], Loss: 0.0252\n",
            "Epoch [9/10], Step [2000/3125], Loss: 0.0269\n",
            "Epoch [9/10], Step [2100/3125], Loss: 0.0293\n",
            "Epoch [9/10], Step [2200/3125], Loss: 0.0217\n",
            "Epoch [9/10], Step [2300/3125], Loss: 0.0247\n",
            "Epoch [9/10], Step [2400/3125], Loss: 0.0207\n",
            "Epoch [9/10], Step [2500/3125], Loss: 0.0217\n",
            "Epoch [9/10], Step [2600/3125], Loss: 0.0291\n",
            "Epoch [9/10], Step [2700/3125], Loss: 0.0219\n",
            "Epoch [9/10], Step [2800/3125], Loss: 0.0180\n",
            "Epoch [9/10], Step [2900/3125], Loss: 0.0340\n",
            "Epoch [9/10], Step [3000/3125], Loss: 0.0280\n",
            "Epoch [9/10], Step [3100/3125], Loss: 0.0198\n",
            "Epoch [9/10], Loss: 0.0002\n",
            "Epoch [10/10], Step [0/3125], Loss: 0.0001\n",
            "Epoch [10/10], Step [100/3125], Loss: 0.0202\n",
            "Epoch [10/10], Step [200/3125], Loss: 0.0162\n",
            "Epoch [10/10], Step [300/3125], Loss: 0.0151\n",
            "Epoch [10/10], Step [400/3125], Loss: 0.0152\n",
            "Epoch [10/10], Step [500/3125], Loss: 0.0152\n",
            "Epoch [10/10], Step [600/3125], Loss: 0.0205\n",
            "Epoch [10/10], Step [700/3125], Loss: 0.0161\n",
            "Epoch [10/10], Step [800/3125], Loss: 0.0147\n",
            "Epoch [10/10], Step [900/3125], Loss: 0.0180\n",
            "Epoch [10/10], Step [1000/3125], Loss: 0.0262\n",
            "Epoch [10/10], Step [1100/3125], Loss: 0.0164\n",
            "Epoch [10/10], Step [1200/3125], Loss: 0.0114\n",
            "Epoch [10/10], Step [1300/3125], Loss: 0.0174\n",
            "Epoch [10/10], Step [1400/3125], Loss: 0.0167\n",
            "Epoch [10/10], Step [1500/3125], Loss: 0.0275\n",
            "Epoch [10/10], Step [1600/3125], Loss: 0.0147\n",
            "Epoch [10/10], Step [1700/3125], Loss: 0.0187\n",
            "Epoch [10/10], Step [1800/3125], Loss: 0.0143\n",
            "Epoch [10/10], Step [1900/3125], Loss: 0.0116\n",
            "Epoch [10/10], Step [2000/3125], Loss: 0.0174\n",
            "Epoch [10/10], Step [2100/3125], Loss: 0.0268\n",
            "Epoch [10/10], Step [2200/3125], Loss: 0.0232\n",
            "Epoch [10/10], Step [2300/3125], Loss: 0.0190\n",
            "Epoch [10/10], Step [2400/3125], Loss: 0.0197\n",
            "Epoch [10/10], Step [2500/3125], Loss: 0.0218\n",
            "Epoch [10/10], Step [2600/3125], Loss: 0.0220\n",
            "Epoch [10/10], Step [2700/3125], Loss: 0.0260\n",
            "Epoch [10/10], Step [2800/3125], Loss: 0.0216\n",
            "Epoch [10/10], Step [2900/3125], Loss: 0.0246\n",
            "Epoch [10/10], Step [3000/3125], Loss: 0.0166\n",
            "Epoch [10/10], Step [3100/3125], Loss: 0.0164\n",
            "Epoch [10/10], Loss: 0.0001\n",
            "Accuracy: 0.9827\n",
            "Precision: 0.9801\n",
            "Recall: 0.9853\n",
            "F1 Score: 0.9827\n",
            "ROC-AUC: 0.9986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10  # Can reduce to 2-3 for CPU testing\n",
        "\n",
        "# 2. Data Preprocessing & Augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),  # Resize to 32x32 (adjust if needed)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 3. Load Dataset (ImageFolder for fake/real folders)\n",
        "train_dir = r'D:\\ML\\minor project\\deepfake image\\train'\n",
        "test_dir = r'D:\\ML\\minor project\\deepfake image\\test'\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# 4. Simple 5-Layer CNN Model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(256 * 1 * 1, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = self.pool(self.relu(self.conv4(x)))\n",
        "        x = self.pool(self.relu(self.conv5(x)))\n",
        "        x = x.view(-1, 256 * 1 * 1)  # Flatten the output\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 5. Model, Loss, Optimizer\n",
        "device = torch.device('cpu')  # Use CPU\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 6. Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "# 7. Evaluation on Test Set\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "y_prob = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_prob.extend(probs.cpu().numpy())\n",
        "\n",
        "# 8. Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "roc_auc = roc_auc_score(y_true, y_prob)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1-Score: {f1:.4f}')\n",
        "print(f'ROC-AUC: {roc_auc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OAeM9gguxHLm",
        "outputId": "a2cfb963-d674-4d4d-f5b4-be03377771c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x000001DA5A7BBF60>\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1582, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\multiprocessing\\process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\multiprocessing\\popen_spawn_win32.py\", line 112, in wait\n",
            "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 72\u001b[0m\n\u001b[0;32m     68\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     70\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 72\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     73\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     74\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[4], line 49\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n\u001b[1;32m---> 49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(x)))\n\u001b[0;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv5(x)))\n\u001b[0;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the output\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}